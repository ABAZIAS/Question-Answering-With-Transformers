{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1133655,"sourceType":"datasetVersion","datasetId":633717}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets evaluate accelerate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T14:40:35.608876Z","iopub.execute_input":"2025-09-05T14:40:35.609617Z","iopub.status.idle":"2025-09-05T14:42:00.278951Z","shell.execute_reply.started":"2025-09-05T14:40:35.609565Z","shell.execute_reply":"2025-09-05T14:42:00.278113Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom datasets import Dataset,load_dataset\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\nfrom transformers import default_data_collator\nimport torch\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-05T14:45:28.999039Z","iopub.execute_input":"2025-09-05T14:45:28.999806Z","iopub.status.idle":"2025-09-05T14:45:29.004937Z","shell.execute_reply.started":"2025-09-05T14:45:28.999772Z","shell.execute_reply":"2025-09-05T14:45:29.004130Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/squad-v11/SQuAD-v1.2.csv')\nds = Dataset.from_pandas(df).train_test_split(test_size=0.05)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T14:45:36.452147Z","iopub.execute_input":"2025-09-05T14:45:36.452436Z","iopub.status.idle":"2025-09-05T14:45:38.012426Z","shell.execute_reply.started":"2025-09-05T14:45:36.452414Z","shell.execute_reply":"2025-09-05T14:45:38.011514Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"MODEL = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\nmodel = AutoModelForQuestionAnswering.from_pretrained(MODEL)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T14:45:47.666344Z","iopub.execute_input":"2025-09-05T14:45:47.666742Z","iopub.status.idle":"2025-09-05T14:45:51.296303Z","shell.execute_reply.started":"2025-09-05T14:45:47.666710Z","shell.execute_reply":"2025-09-05T14:45:51.295490Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d71d30ffcb8846c698f70d07bd453b42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c44822b3ebd4b76bb62b93ed23e6f27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf66c6ea3b6448479ba8237e077515a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccf470f1f3614e4e96f0546fd103139e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c73d1136bc7747bdb48b2faa2f4689fe"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"max_length = 384\ndoc_stride = 128\n\ndef prepare_features(examples):\n    # tokenize with sliding window + offsets\n    tokenized = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_map = tokenized.pop(\"overflow_to_sample_mapping\")\n    offsets = tokenized.pop(\"offset_mapping\")\n\n    starts, ends = [], []\n\n    # helper to extract (text, start) for the original example index\n    def _get_answer(sample_idx):\n        # 1) HF style: examples may contain 'answers' (batched -> list of dicts)\n        if \"answers\" in examples and examples[\"answers\"] is not None:\n            ans_entry = examples[\"answers\"][sample_idx]\n            # ans_entry expected to be a dict with 'text' and 'answer_start' (which may be lists)\n            if isinstance(ans_entry, dict):\n                texts = ans_entry.get(\"text\", [])\n                starts_list = ans_entry.get(\"answer_start\", [])\n                text = texts[0] if isinstance(texts, (list, tuple)) and texts else (texts if texts else None)\n                start = starts_list[0] if isinstance(starts_list, (list, tuple)) and starts_list else (starts_list if starts_list else None)\n                return text, start\n\n        # 2) CSV style: separate columns 'answer' and 'answer_start' (batched -> lists)\n        ans_col = examples.get(\"answer\")\n        ans_start_col = examples.get(\"answer_start\")\n        if ans_col is not None and ans_start_col is not None:\n            return ans_col[sample_idx], ans_start_col[sample_idx]\n\n        # 3) not found\n        return None, None\n\n    for i, offset in enumerate(offsets):\n        sample_idx = sample_map[i]\n        answer_text, answer_start = _get_answer(sample_idx)\n\n        if answer_text is None or answer_start is None:\n            starts.append(0); ends.append(0); continue\n\n        answer_start = int(answer_start)\n        answer_end = answer_start + len(answer_text)\n\n        seq_ids = tokenized.sequence_ids(i)\n        # find context token span\n        context_start = next(j for j, s in enumerate(seq_ids) if s == 1)\n        context_end = len(seq_ids) - 1 - next(j for j, s in enumerate(reversed(seq_ids)) if s == 1)\n\n        # if answer not fully inside this window -> CLS token (0)\n        if not (answer_start >= offset[context_start][0] and answer_end <= offset[context_end][1]):\n            starts.append(0); ends.append(0)\n        else:\n            ts = context_start\n            while ts <= context_end and offset[ts][0] <= answer_start:\n                ts += 1\n            te = context_end\n            while te >= context_start and offset[te][1] >= answer_end:\n                te -= 1\n            starts.append(ts - 1)\n            ends.append(te + 1)\n\n    tokenized[\"start_positions\"] = starts\n    tokenized[\"end_positions\"] = ends\n    return tokenized\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T14:46:54.436404Z","iopub.execute_input":"2025-09-05T14:46:54.437331Z","iopub.status.idle":"2025-09-05T14:46:54.455534Z","shell.execute_reply.started":"2025-09-05T14:46:54.437293Z","shell.execute_reply":"2025-09-05T14:46:54.454142Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"train = ds[\"train\"].select(range(4000))   \nvalid = ds[\"test\"].select(range(500))\ntokenized_train = train.map(prepare_features, batched=True, remove_columns=train.column_names)\ntokenized_valid = valid.map(prepare_features, batched=True, remove_columns=valid.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T14:47:06.301549Z","iopub.execute_input":"2025-09-05T14:47:06.302259Z","iopub.status.idle":"2025-09-05T14:47:08.677253Z","shell.execute_reply.started":"2025-09-05T14:47:06.302227Z","shell.execute_reply":"2025-09-05T14:47:08.676494Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"321177577faa4ec9a871d819f71f259c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e929e164308a410da5aedd526325ec8f"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=\"qa-run\",\n    num_train_epochs=1,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    logging_steps=200,\n    save_strategy=\"no\",\n    fp16=torch.cuda.is_available(),\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_valid,\n    data_collator=default_data_collator,\n    tokenizer=tokenizer,\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T15:07:00.359189Z","iopub.execute_input":"2025-09-05T15:07:00.360179Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/127316169.py:11: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null}]}